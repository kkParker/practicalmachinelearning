---
title: "Course Project Write Up"
author: "Coursera KKParker"
date: "December 22, 2015"
output: html_document
---
#Executive Summary
In this report, a human activity recognition dataset was used to predict a weight lifting activity by a user.  The data were split in to training and testing data sets.  Predictors were pruned in order to remove variables that did not have a variety or values and those that were missing a lot of values.  In addition, the variables were standardized.  A CART model, bagging tree model, and linear discriminant model were used to predict the activity class.  The Bagging tree model proved to be the best for this dataset with a 99.9% accuracy on the training dataset.  In addition, all 20 of the test dataset points were accurately predicted.

#Introduction
With the increased used of activity trackers such as the Fit Bit and Vivofit, we have seen an increase in the ability of a computer to recognize and classify what a human is doing.  This area of study is called *human activity recognition*. The trackers collect data such as vertical and horizonal movements and try to classify subjects as sleeping, walking, running, or walking up the stairs.

#Data Set
This study examines a dataset from a study by Velloso, Bulling, Gellersen, Ugulino, and Fuks (see citation at the end of the report: http://groupware.les.inf.puc-rio.br/har#ixzz3v4M2P3qT). It is weight lifting data as a study .  In the study, six subjects performed 10 replications of five different bicep curls:

*A-correctly  
*B-throwing elbows to front  
*C-lifting dumbbell only half way  
*D-lowering dumbbell only half way  
*E-throwing hips to front    

In our analysis, the variable classe contains this target variable for our model which holds the values: A, B, C, D or E.

##Loading the data
Let's begin by reading in the Training and Testing Data.
```{r cache=TRUE}
pmltrain <- read.csv('C:/Temp/pml-training.csv')
pmltest <- read.csv('C:/Temp/pml-testing.csv')
dim(pmltrain)
```
We can see we have 160 variables and there are 19,622 observations or recordings from these subjects performing these activities.

#Setting up r and the environment
Let's set up markdown to cache the r code to help with the speed of running our program.
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
We will begin by loading the caret and kernlab packages. The caret package (Classification and Regression Training) allows us to perform predictive modeling as well as has options for preprocessing, model tuning with resampling, and variable importance estimation. 
```{r warning=FALSE}
library(caret); library(kernlab)
```

##Removing variables with little variance
Now, let's look for variables with little or no variance. If the variables are mostly the same values, then they will not help us predict the activity classe.  We will use caret's nearZeroVar function that identifies variables with little variance.
```{r}
nzv <- nearZeroVar(pmltrain,saveMetrics=TRUE)
removenzvvar <- row.names(subset(nzv,nzv == TRUE))
length(removenzvvar)
```
So we have 60 variables with little variance, so let's begin our preprocessing by removing these variables from contention as predictors so we have 100 variables left in the data set.
```{r}
pmltrain2 <- pmltrain[,!names(pmltrain) %in% removenzvvar]
```

##Examining the target variable
Next, let's begin to look at our target variable, the classe.
```{r}
table(pmltrain2$classe)
```
From this table we can see there are 5 different classes as we expected.

##Examining the remainder of our predictors
Next, let's start to examine our predictors. X is an index so we won't use it as a predictor
```{r}
table(pmltrain2$user_name)
```
We can see we have 6 users who are being monitored in the user_name column. This is probably very useful information.  We have between 2610 to 3892 observations across users. Let's see how each person distributes across the classes

```{r}
table(pmltrain2$user_name,pmltrain2$classe)
plot(pmltrain2$classe~pmltrain2$user_name, xlab="User Name", ylab="Activity Classe")
```

We can see that classe A (correct execution) had the largest proportion for all users and they did all complete all classes of activities.

Lets ignore the 3 time variables (raw_timestamp_part_1, raw_timestamp_part_2, and cvtd_timestamp) since we can't use them with our test data since we have single observations.


What is num_window?
```{r}
plot(pmltrain2$X,pmltrain2$num_window,col=pmltrain2$user_name,main="color=user_name", xlab="Time", ylab="num_window")
plot(pmltrain2$X,pmltrain2$num_window,col=pmltrain2$classe, main="color=classe",xlab="Time", ylab="num_window")
```

This variable num_window appears to be a counter so I will not include it as a predictor. So let's remove the 3 time variables and num_window.

```{r}
pmltrain2 <- pmltrain2[,!names(pmltrain2) %in% c("num_window", "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp")]
```

##Removing Missing Values
The remainder of the variables appear to be the measurements so we will now attempt to look for variables with too many missing values.
```{r}
M <- sapply(pmltrain2, function(x) sum(is.na(x))) 
removemissvar <- names(M[M>0])
length(removemissvar)
pmltrain2 <- pmltrain2[,!names(pmltrain2) %in% removemissvar]
```
We have removed the 41 variables with too many missing values. This leaves use with 53 variables that we will use as predictors.  These variables were taken from 4 sensors on the user and the dumbell.  In the dataset they are represented by the following:

*sensor glove: _forearm  
*sensor armband: _arm  
*sensor lumbar belt: _belt  
*sensor dumbbell: _dumbbell  

##Creating the training dataset
We will use ptrain as our training dataset. It has 53 numeric and integer variables and it also has the target variable classe. Note we are removing the X index variable.

```{r}
ptrain <- subset(pmltrain2, select=-c(X))
rownames(ptrain) <- 1:nrow(ptrain)
```

##Standardizing the numeric predictors
We know that standardizing the numeric data will help with prediction.  So let's standardize the numeric and integer variables (all those left except user_name and classe).

```{r}
ptrainvarnum <- subset(ptrain,select=-c(user_name,classe))
preObjStd <- preProcess(ptrainvarnum,method=c("center","scale"))
ptrainpreprocStd <- predict(preObjStd,ptrainvarnum)
ptrainwStd <- data.frame(cbind(data.frame(ptrain$classe),data.frame(ptrain$user_name)))
names(ptrainwStd) <- c("classe","user_name")
ptrainwStd <- cbind(ptrainwStd,ptrainpreprocStd)
head(ptrain$roll_belt)
head(ptrainwStd$roll_belt)
```
We can see how roll_belt is different for the original training and standardized training datasets.

#Model Building


##CART model (recursiving partitioning)
Let's try to fit a CART classification model predicting classe with the 53 remaining variables.

```{r warning=FALSE}
library(rpart)
modFit_rpart<- train(classe ~., data=ptrain, method="rpart")
modFit_rpart
predictions <- predict(modFit_rpart, newdata=ptrain)
confusionMatrix(predictions,ptrain$classe)
```
##Bagging Model
Let's see if we can improve the model with the bagging option since the accuracy is quite poor at 49%.
```{r warning=FALSE}
library(ipred)
modFit_treebag<- train(classe ~., data=ptrain, method="treebag")
modFit_treebag
predictions <- predict(modFit_treebag, newdata=ptrain)
confusionMatrix(predictions,ptrain$classe)
```
The accuracy of this model is remarkably better than the simple CART model at 99.9% for our training data.

##Linear Discriminant Analysis
Just for reference, let's also try a Linear Discriminant Model.
```{r warning=FALSE}
modFit_lda<- train(classe ~., data=ptrain, method="lda")
modFit_lda
predictions <- predict(modFit_lda, newdata=ptrain)
confusionMatrix(predictions,ptrain$classe)
```
The accuracy of 73% for this model is better than CART but quite a bit worse than the tree bagging model.  But, we know that standardizing variables can help some models perform better, so let's use our standardized variables in the LDA model.

##Linear Discriminant Analysis with standardized variables
And now let's see if using the standardized variables helps the prediction accuracy.
```{r}
modFit_Std_lda<- train(classe ~., data=ptrainwStd, method="lda")
modFit_Std_lda
predictions <- predict(modFit_Std_lda, newdata=ptrainwStd)
confusionMatrix(predictions,ptrainwStd$classe)
```
The standardization did not help the predictions in this case.  Our best model was using the treebagging approach.  


#Cross-Validation and estimation of out-of-sample error

Let's use our tree bagging approach and perform 10-fold cross 
validation.
```{r warning=FALSE}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
modFit_CV_treebag<- train(classe ~., data=ptrain,             method="treebag",trControl = fitControl)
modFit_CV_treebag
predictions <- predict(modFit_CV_treebag, newdata=ptrain)
confusionMatrix(predictions,ptrain$classe)
```

##Out-of-sample Error Rate
We can see this model has high accuracy of 99.9% on the test data.  This is an average of 100 resampled models, so we can estimate the out-of-sample error rate by taking 1 - accuracy.
```{r}
print(paste('Out-of-Sample Error Rate',1 - modFit_CV_treebag$results[2]))
```


Let's see which predictor variables it is using as most important.
```{r}
varImp(modFit_CV_treebag)
```
From this analysis we can see the top 3 variables were measures from the lumbar belt.

#Predicting the Test Values
Since we are happy with this final model, we will use it to predict our test data values.
```{r}
predictions <- predict(modFit_CV_treebag, newdata=pmltest)
answers <- as.character(predictions)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
setwd('C:/Temp')
pml_write_files(answers)
```
The 20 test data values were checked against the correct values and all 20 were correct using this bagging tree model with 10-fold cross validation.

NOTE: The data was from:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

http://groupware.les.inf.puc-rio.br/har

